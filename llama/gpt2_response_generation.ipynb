{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/bk123477/decs_jupyter_lab/bartvenv/lib/python3.7/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "#get transformers\n",
    "from transformers import GPT2LMHeadModel, GPT2Tokenizer\n",
    "\n",
    "#get large GPT2 tokenizer and GPT2 model\n",
    "tokenizer = GPT2Tokenizer.from_pretrained(\"gpt2\")\n",
    "model = GPT2LMHeadModel.from_pretrained(\"gpt2\", pad_token_id=tokenizer.eos_token_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import transformers\n",
    "from transformers import GPT2LMHeadModel, GPT2Tokenizer, Trainer, TrainingArguments, DataCollatorForLanguageModeling\n",
    "\n",
    "import json\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "import re\n",
    "from pprint import pprint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch, gc\n",
    "gc.collect()\n",
    "torch.cuda.empty_cache()\n",
    "\n",
    "import os\n",
    "os.environ['CUDA_VISIBLE_DEVICES'] = \"5\" # nvidia-smi로 비어있는 gpu 확인하고 여기서 선택할것!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    }
   ],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_lists = []\n",
    "# label_lists = []\n",
    "# with open(\"test_session_2.jsonl\") as f:\n",
    "#     for json_line in f:\n",
    "#         current_strs = \"\"\n",
    "#         prev_strs = \"\"\n",
    "#         json_file = json.loads(json_line)\n",
    "#         current_conversation = json_file[\"dialog\"]\n",
    "#         for item1 in current_conversation:\n",
    "#             current_strs += f\"{item1['text']} \" \n",
    "#         train_lists.append(current_strs)\n",
    "#         prev_conversation = json_file[\"previous_dialogs\"]\n",
    "#         previous_dialogue =[item['dialog'] for item in prev_conversation]\n",
    "#         for item2 in previous_dialogue[0]:\n",
    "#             prev_strs += f\"{item2['text']} \"\n",
    "#         train_lists.append(prev_strs)\n",
    "        \n",
    "# lengdata = len(train_lists)\n",
    "\n",
    "train_lists = []\n",
    "label_lists = []\n",
    "with open(\"test_session_2.jsonl\") as f:\n",
    "    for json_line in f:\n",
    "        current_strs = \"\"\n",
    "        prev_strs = \"\"\n",
    "        json_file = json.loads(json_line)\n",
    "        current_conversation = json_file[\"dialog\"]\n",
    "        for i, dic in enumerate(current_conversation):\n",
    "            if i == 0:\n",
    "                current_strs = f\"Speaker {i%2 + 1} : \"+ dic[\"text\"] + \" \"               \n",
    "            if i != 0 and i != len(current_conversation)-1:\n",
    "                train_lists.append(current_strs)\n",
    "                current_strs += f\"Speaker {i%2 + 1} : \"+ dic[\"text\"] + \" \"\n",
    "                label_lists.append(dic[\"text\"])\n",
    "            if i == len(current_conversation)-1:\n",
    "                train_lists.append(current_strs)\n",
    "                label_lists.append(dic[\"text\"])\n",
    "                \n",
    "lengdata = len(train_lists)\n",
    "print(train_lists[0])\n",
    "print(label_lists[0])\n",
    "print(train_lists[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "286257\n"
     ]
    }
   ],
   "source": [
    "file_name2 = \"bart_all_session_no_persona/session_2/train.jsonl\"\n",
    "\n",
    "train_lists = []\n",
    "label_lists = []\n",
    "\n",
    "with open(file_name2) as f:\n",
    "    for json_line in f:\n",
    "        current_strs = \"\"\n",
    "        prev_strs = \"\"\n",
    "        # print(json_line)\n",
    "        json_file = json.loads(json_line)\n",
    "        current_conversation = json_file[\"dialog\"]\n",
    "        for i, dic in enumerate(current_conversation):\n",
    "            if i == 0:\n",
    "                current_strs = f\"Speaker {i%2 + 1} : \"+ dic[\"text\"] + \" \"\n",
    "            if i != 0 and i != len(current_conversation)-1:\n",
    "                train_lists.append(current_strs)\n",
    "                current_strs += f\"Speaker {i%2 + 1} : \"+ dic[\"text\"] + \" \"\n",
    "                label_lists.append(dic[\"text\"])\n",
    "            if i == len(current_conversation)-1:\n",
    "                train_lists.append(current_strs)\n",
    "                label_lists.append(dic[\"text\"])\n",
    "\n",
    "        prev_dialogs = json_file['previous_dialogs'][0]['dialog']\n",
    "        for i, dic in enumerate(prev_dialogs):\n",
    "            if i == 0:\n",
    "                prev_strs = f\"Speaker {i%2 + 1} : \"+ dic['text'] + \" \"\n",
    "            if i != 0 and i != len(prev_dialogs)-1:\n",
    "                train_lists.append(prev_strs)\n",
    "                prev_strs += f\"Speaker {i%2 + 1} : \"+ dic['text'] + \" \"\n",
    "                label_lists.append(dic['text'])\n",
    "            if i == len(prev_dialogs)-1:\n",
    "                train_lists.append(prev_strs)\n",
    "                label_lists.append(dic[\"text\"])\n",
    "\n",
    "file_name3 = \"bart_all_session_no_persona/session_3/train.jsonl\"\n",
    "with open(file_name3) as f:\n",
    "    for json_line in f:\n",
    "        current_strs = \"\"\n",
    "        prev_strs = \"\"\n",
    "        # print(json_line)\n",
    "        json_file = json.loads(json_line)\n",
    "        current_conversation = json_file[\"dialog\"]\n",
    "\n",
    "        for i, dic in enumerate(current_conversation):\n",
    "            if i == 0:\n",
    "                current_strs = f\"Speaker {i%2 + 1} : \"+ dic[\"text\"] + \" \"\n",
    "            if i != 0 and i != len(current_conversation)-1:\n",
    "                train_lists.append(current_strs)\n",
    "                current_strs += f\"Speaker {i%2 + 1} : \"+ dic[\"text\"] + \" \"\n",
    "                label_lists.append(dic[\"text\"])\n",
    "            if i == len(current_conversation)-1:\n",
    "                train_lists.append(current_strs)\n",
    "                label_lists.append(dic[\"text\"])\n",
    "\n",
    "        prev_prev_dialogs = json_file['previous_dialogs'][0]['dialog']\n",
    "        for i, dic in enumerate(prev_prev_dialogs):\n",
    "            if i == 0:\n",
    "                prev_strs = f\"Speaker {i%2 + 1} : \"+ dic['text'] + \" \"\n",
    "            if i != 0 and i != len(prev_prev_dialogs)-1:\n",
    "                train_lists.append(prev_strs)\n",
    "                prev_strs += f\"Speaker {i%2 + 1} : \"+ dic['text'] + \" \"\n",
    "                label_lists.append(dic['text'])\n",
    "            if i == len(prev_prev_dialogs)-1:\n",
    "                train_lists.append(prev_strs)\n",
    "                label_lists.append(dic[\"text\"])\n",
    "\n",
    "        prev_dialogs = json_file['previous_dialogs'][1]['dialog']\n",
    "        for i, dic in enumerate(prev_dialogs):\n",
    "            if i == 0:\n",
    "                prev_strs = f\"Speaker {i%2 + 1} : \"+ dic['text'] + \" \"\n",
    "            if i != 0 and i != len(prev_dialogs)-1:\n",
    "                train_lists.append(prev_strs)\n",
    "                prev_strs += f\"Speaker {i%2 + 1} : \"+ dic['text'] + \" \"\n",
    "                label_lists.append(dic['text'])\n",
    "            if i == len(prev_dialogs)-1:\n",
    "                train_lists.append(prev_strs)\n",
    "                label_lists.append(dic[\"text\"])\n",
    "\n",
    "file_name4 = \"bart_all_session_no_persona/session_4/train.jsonl\"\n",
    "with open(file_name4) as f:\n",
    "    for json_line in f:\n",
    "        current_strs = \"\"\n",
    "        prev_strs = \"\"\n",
    "        # print(json_line)\n",
    "        json_file = json.loads(json_line)\n",
    "        current_conversation = json_file[\"dialog\"]\n",
    "\n",
    "        for i, dic in enumerate(current_conversation):\n",
    "            if i == 0:\n",
    "                current_strs = f\"Speaker {i%2 + 1} : \"+ dic[\"text\"] + \" \"\n",
    "            if i != 0 and i != len(current_conversation)-1:\n",
    "                train_lists.append(current_strs)\n",
    "                current_strs += f\"Speaker {i%2 + 1} : \"+ dic[\"text\"] + \" \"\n",
    "                label_lists.append(dic[\"text\"])\n",
    "            if i == len(current_conversation)-1:\n",
    "                train_lists.append(current_strs)\n",
    "                label_lists.append(dic[\"text\"])\n",
    "\n",
    "        prev_prev_prev_dialogs = json_file['previous_dialogs'][0]['dialog']\n",
    "        for i, dic in enumerate(prev_prev_prev_dialogs):\n",
    "            if i == 0:\n",
    "                prev_strs = f\"Speaker {i%2 + 1} : \"+ dic['text'] + \" \"\n",
    "            if i != 0 and i != len(prev_prev_prev_dialogs)-1:\n",
    "                train_lists.append(prev_strs)\n",
    "                prev_strs += f\"Speaker {i%2 + 1} : \"+ dic['text'] + \" \"\n",
    "                label_lists.append(dic['text'])\n",
    "            if i == len(prev_prev_prev_dialogs)-1:\n",
    "                train_lists.append(prev_strs)\n",
    "                label_lists.append(dic[\"text\"])\n",
    "\n",
    "        prev_prev_dialogs = json_file['previous_dialogs'][1]['dialog']\n",
    "        for i, dic in enumerate(prev_prev_dialogs):\n",
    "            if i == 0:\n",
    "                prev_strs = f\"Speaker {i%2 + 1} : \"+ dic['text'] + \" \"\n",
    "            if i != 0 and i != len(prev_prev_dialogs)-1:\n",
    "                train_lists.append(prev_strs)\n",
    "                prev_strs += f\"Speaker {i%2 + 1} : \"+ dic['text'] + \" \"\n",
    "                label_lists.append(dic['text'])\n",
    "            if i == len(prev_prev_dialogs)-1:\n",
    "                train_lists.append(prev_strs)\n",
    "                label_lists.append(dic[\"text\"])\n",
    "\n",
    "        prev_dialogs = json_file['previous_dialogs'][2]['dialog']\n",
    "        for i, dic in enumerate(prev_dialogs):\n",
    "            if i == 0:\n",
    "                prev_strs = f\"Speaker {i%2 + 1} : \"+ dic['text'] + \" \"\n",
    "            if i != 0 and i != len(prev_dialogs)-1:\n",
    "                train_lists.append(prev_strs)\n",
    "                prev_strs += f\"Speaker {i%2 + 1} : \"+ dic['text'] + \" \"\n",
    "                label_lists.append(dic['text'])\n",
    "            if i == len(prev_dialogs)-1:\n",
    "                train_lists.append(prev_strs)\n",
    "                label_lists.append(dic[\"text\"])\n",
    "lengdata = len(train_lists)\n",
    "print(lengdata)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "valid_lists = train_lists[int(lengdata*0.7):int(lengdata*0.9)]\n",
    "valid_label_lists = label_lists[int(lengdata*0.7):int(lengdata*0.9)]\n",
    "\n",
    "test_lists = train_lists[int(lengdata*0.9):]\n",
    "test_label_lists = label_lists[int(lengdata*0.9):]\n",
    "\n",
    "train_lists = train_lists[:int(lengdata*0.7)]\n",
    "train_label_lists = label_lists[:int(lengdata*0.7)]\n",
    "MAX_INPUT_LENGTH = 1024\n",
    "MAX_TARGET_LENGTH = 512\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "encodings = tokenizer(\"\\n\\n\".join(valid_lists), return_tensors='pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import math\n",
    "# def score(sentence):\n",
    "#     tokenize_input = tokenizer.tokenize(sentence)\n",
    "#     tensor_input = torch.tensor([tokenizer.convert_tokens_to_ids(tokenize_input)])\n",
    "#     loss = model(tensor_input, labels = tensor_input)\n",
    "#     return math.exp(loss.loss.item())\n",
    "\n",
    "# # print([score(i) for i in test_lists])\n",
    "# for i in test_lists:\n",
    "#     print(score(i))\n",
    "#     break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████▉| 18114/18116 [12:28<00:00, 24.21it/s]\n"
     ]
    }
   ],
   "source": [
    "import math\n",
    "import statistics\n",
    "\n",
    "\n",
    "model = GPT2LMHeadModel.from_pretrained(\"gpt2\", pad_token_id=tokenizer.eos_token_id).to(device)\n",
    "max_length = model.config.n_positions\n",
    "stride = 512\n",
    "seq_len = encodings.input_ids.size(1)\n",
    "\n",
    "nlls = []\n",
    "prev_end_loc = 0\n",
    "for begin_loc in tqdm(range(0, seq_len, stride)):\n",
    "    end_loc = min(begin_loc + max_length, seq_len)\n",
    "    trg_len = end_loc - prev_end_loc  # may be different from stride on last loop\n",
    "    input_ids = encodings.input_ids[:, begin_loc:end_loc].to(device)\n",
    "    target_ids = input_ids.clone()\n",
    "    target_ids[:, :-trg_len] = -100\n",
    "\n",
    "    with torch.no_grad():\n",
    "        outputs = model(input_ids, labels=target_ids)\n",
    "        \n",
    "        # loss is calculated using CrossEntropyLoss which averages over valid labels\n",
    "        # N.B. the model only calculates loss over trg_len - 1 labels, because it internally shifts the labels\n",
    "        # to the left by 1.\n",
    "        neg_log_likelihood = outputs.loss\n",
    "\n",
    "    nlls.append(neg_log_likelihood)\n",
    "\n",
    "    prev_end_loc = end_loc\n",
    "    if end_loc == seq_len:\n",
    "        break\n",
    "\n",
    "# ppl = math.exp(torch.stack(nlls).mean()) * 10\n",
    "ppl = math.exp(torch.stack(nlls).mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Perplexity :  17.73265055126779\n"
     ]
    }
   ],
   "source": [
    "print(\"Perplexity : \", ppl)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import set_seed\n",
    "from rouge import Rouge\n",
    "from evaluate import load\n",
    "perplexity = load(\"perplexity\", module_type=\"metric\")\n",
    "results = 0\n",
    "rouge = Rouge()\n",
    "# set seed to reproduce results. Feel free to change the seed though to get different results\n",
    "set_seed(42)\n",
    "\n",
    "filename = \"./output_msc_2_gpt_no_persona.txt\"\n",
    "total_f1 = 0\n",
    "pred_arr = []\n",
    "ref_arr = []\n",
    "\n",
    "for i in range(len(test_lists)):\n",
    "    model_inputs = tokenizer(test_lists[i], return_tensors='pt')\n",
    "\n",
    "    # set top_k = 50 and set top_p = 0.95 and num_return_sequences = 3\n",
    "    sample_outputs = model.generate(\n",
    "        **model_inputs,\n",
    "        max_new_tokens=40,\n",
    "        do_sample=True,\n",
    "        top_k=50,\n",
    "        top_p=0.95, \n",
    "    ) \n",
    "    generation_text = tokenizer.decode(sample_outputs[0], skip_special_tokens=True)\n",
    "    pred_arr.append(generation_text[len(test_lists[i])-1:])\n",
    "    with open(filename, \"a\") as f:\n",
    "        f.write(f\"\\nprediction: \\n{generation_text[len(test_lists[i])-1:]}\\ngt     : \\n{test_label_lists[i]}\")\n",
    "    total_f1 += rouge.get_scores(generation_text[len(test_lists[i])-1:], test_label_lists[i], avg=True)['rouge-1']['f']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''rouge랑 perplexity를 다시 구하는 코드'''\n",
    "total_f1 /= len(test_lists)\n",
    "print(\"Rouge : \", total_f1)\n",
    "results = perplexity.compute(predictions=pred_arr, model_id='gpt2')\n",
    "print(\"Perplexity : \", round(results[\"mean_perplexity\"], 2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.translate.bleu_score import sentence_bleu\n",
    "bleu = 0\n",
    "\n",
    "for i in range(len(test_lists)):\n",
    "    model_inputs = tokenizer(test_lists[i], return_tensors='pt')\n",
    "\n",
    "    # set top_k = 50 and set top_p = 0.95\n",
    "    sample_outputs = model.generate(\n",
    "        **model_inputs,\n",
    "        max_new_tokens=40,\n",
    "        do_sample=True,\n",
    "        top_k=50,\n",
    "        top_p=0.95, \n",
    "    ) \n",
    "    generation_text = tokenizer.decode(sample_outputs[0], skip_special_tokens=True)\n",
    "    bleu += sentence_bleu(test_label_lists[i], generation_text[len(test_lists[i])-1:], weights=(1,0,0,0))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bleu /= len(test_lists)\n",
    "print(\"BLEU : \", bleu)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "'git'��(��) ���� �Ǵ� �ܺ� ����, ������ �� �ִ� ���α׷�, �Ǵ�\n",
      "��ġ ������ �ƴմϴ�.\n"
     ]
    }
   ],
   "source": [
    "!pip install git"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'pytreebank'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mpytreebank\u001b[39;00m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;66;03m# load the sentiment treebank corpus in the parenthesis format,\u001b[39;00m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;66;03m# e.g. \"(4 (2 very ) (3 good))\"\u001b[39;00m\n\u001b[0;32m      4\u001b[0m dataset \u001b[38;5;241m=\u001b[39m pytreebank\u001b[38;5;241m.\u001b[39mload_sst()\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'pytreebank'"
     ]
    }
   ],
   "source": [
    "import pytreebank\n",
    "# load the sentiment treebank corpus in the parenthesis format,\n",
    "# e.g. \"(4 (2 very ) (3 good))\"\n",
    "dataset = pytreebank.load_sst()\n",
    "# add Javascript and CSS to the Ipython notebook\n",
    "pytreebank.LabeledTree.inject_visualization_javascript()\n",
    "# select and example to visualize\n",
    "example = dataset[\"train\"][0]\n",
    "# display it in the page\n",
    "example.display()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
